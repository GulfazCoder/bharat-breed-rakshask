# üöÄ Immediate AI Training Setup Guide
## From 75% to 100% Accuracy in 6 Months

This guide provides actionable steps to implement the AI training strategy immediately.

## üéØ Phase 1: Enhanced Current System (This Week - 85% Target)

### Step 1: Implement Enhanced Classification Service

The enhanced service is already created. To integrate it:

1. **Update the classify page to use enhanced service:**

```typescript
// In src/app/classify/page.tsx
import { enhancedAIService } from '@/lib/services/enhanced-ai-classification';

// Replace the classification call:
const result = await enhancedAIService.classifyImageEnhanced(
  imageData, 
  userLocation, // Get from browser geolocation
  {
    suspectedBreed: farmerGuess, // From user input
    animalAge: estimatedAge,
    purpose: 'dairy' // or 'draught', 'meat'
  }
);
```

2. **Add user location detection:**

```javascript
// Get user location for regional bias
navigator.geolocation.getCurrentPosition((position) => {
  const { latitude, longitude } = position.coords;
  // Convert to region name using reverse geocoding
  enhancedAIService.setUserLocation('gujarat'); // Example
});
```

3. **Add feedback collection:**

```typescript
// When user corrects classification
enhancedAIService.addUserFeedback(
  imageData,
  originalPrediction,
  userCorrection,
  confidence,
  userLocation
);
```

### Step 2: Data Collection Infrastructure

Create these components immediately:

#### A. Mobile Data Collection Component
```typescript
// src/components/DataCollection.tsx
import { useState } from 'react';

export function DataCollectionForm({ imageData, onSubmit }) {
  const [annotation, setAnnotation] = useState({
    breed: '',
    animalType: '',
    age: '',
    gender: '',
    health: '',
    location: '',
    farmerName: '',
    phoneNumber: '',
    notes: ''
  });

  return (
    <form onSubmit={(e) => {
      e.preventDefault();
      onSubmit(annotation);
    }}>
      <div className="space-y-4">
        <input
          type="text"
          placeholder="Breed Name"
          value={annotation.breed}
          onChange={(e) => setAnnotation({...annotation, breed: e.target.value})}
          required
        />
        
        <select 
          value={annotation.animalType}
          onChange={(e) => setAnnotation({...annotation, animalType: e.target.value})}
        >
          <option value="">Select Animal Type</option>
          <option value="cattle">Cattle</option>
          <option value="buffalo">Buffalo</option>
        </select>

        <select 
          value={annotation.age}
          onChange={(e) => setAnnotation({...annotation, age: e.target.value})}
        >
          <option value="">Select Age</option>
          <option value="calf">Calf (0-1 year)</option>
          <option value="young">Young (1-3 years)</option>
          <option value="adult">Adult (3-8 years)</option>
          <option value="mature">Mature (8+ years)</option>
        </select>

        <textarea
          placeholder="Additional notes (health, special characteristics)"
          value={annotation.notes}
          onChange={(e) => setAnnotation({...annotation, notes: e.target.value})}
        />

        <button type="submit" className="btn-primary w-full">
          Submit Training Data
        </button>
      </div>
    </form>
  );
}
```

#### B. Expert Validation Interface
```typescript
// src/components/ExpertValidation.tsx
export function ExpertValidationInterface() {
  const [pendingImages, setPendingImages] = useState([]);
  
  const validateClassification = async (imageId, isCorrect, correction?) => {
    await fetch('/api/expert-validation', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        imageId,
        isCorrect,
        correction,
        expertId: localStorage.getItem('expertId'),
        timestamp: new Date().toISOString()
      })
    });
  };

  return (
    <div className="expert-validation-interface">
      {pendingImages.map(image => (
        <div key={image.id} className="validation-card">
          <img src={image.url} alt="Livestock" />
          <div className="prediction">
            <p>AI Prediction: {image.prediction}</p>
            <p>Confidence: {image.confidence}%</p>
          </div>
          <div className="validation-actions">
            <button 
              onClick={() => validateClassification(image.id, true)}
              className="btn-success"
            >
              ‚úÖ Correct
            </button>
            <button 
              onClick={() => {
                const correction = prompt('Enter correct breed:');
                validateClassification(image.id, false, correction);
              }}
              className="btn-error"
            >
              ‚ùå Incorrect
            </button>
          </div>
        </div>
      ))}
    </div>
  );
}
```

## üéØ Phase 2: Data Collection Pipeline (Week 2-4)

### Step 1: Setup Training Data API

Create API endpoints to collect training data:

```typescript
// pages/api/training-data.ts
import { NextApiRequest, NextApiResponse } from 'next';
import { writeFileSync } from 'fs';
import path from 'path';

export default async function handler(req: NextApiRequest, res: NextApiResponse) {
  if (req.method === 'POST') {
    const { imageData, annotation, metadata } = req.body;
    
    // Generate unique filename
    const timestamp = Date.now();
    const filename = `training_${timestamp}.json`;
    
    // Prepare training record
    const trainingRecord = {
      id: timestamp,
      imageData: imageData, // Base64 encoded image
      annotation: {
        breed: annotation.breed,
        animalType: annotation.animalType,
        age: annotation.age,
        gender: annotation.gender,
        health: annotation.health,
        bodyCondition: annotation.bodyCondition,
        location: annotation.location
      },
      metadata: {
        captureDate: new Date().toISOString(),
        deviceInfo: req.headers['user-agent'],
        farmerInfo: metadata.farmerInfo,
        weatherConditions: metadata.weather,
        lightingConditions: metadata.lighting,
        imageQuality: metadata.quality
      },
      validation: {
        status: 'pending', // pending, validated, rejected
        validatedBy: null,
        validationDate: null,
        confidence: annotation.confidence || 1.0
      }
    };
    
    // Save to training data directory
    const trainingDataPath = path.join(process.cwd(), 'data', 'training', filename);
    writeFileSync(trainingDataPath, JSON.stringify(trainingRecord, null, 2));
    
    res.status(200).json({ success: true, id: timestamp });
  } else {
    res.status(405).json({ error: 'Method not allowed' });
  }
}
```

### Step 2: Quality Control Pipeline

```python
# scripts/quality_control.py
import json
import os
from PIL import Image
import base64
import io

def validate_training_data():
    """Validate and clean training data"""
    training_dir = 'data/training'
    validated_dir = 'data/validated'
    rejected_dir = 'data/rejected'
    
    for filename in os.listdir(training_dir):
        if filename.endswith('.json'):
            filepath = os.path.join(training_dir, filename)
            
            with open(filepath, 'r') as f:
                record = json.load(f)
            
            # Quality checks
            quality_score = check_data_quality(record)
            
            if quality_score >= 0.8:
                # Move to validated
                move_to_validated(record, filename)
                print(f"‚úÖ {filename} - Quality Score: {quality_score}")
            else:
                # Move to rejected
                move_to_rejected(record, filename)
                print(f"‚ùå {filename} - Quality Score: {quality_score}")

def check_data_quality(record):
    """Calculate quality score for training record"""
    score = 0.0
    
    # Image quality (30%)
    if check_image_quality(record['imageData']):
        score += 0.3
    
    # Annotation completeness (40%)
    if record['annotation']['breed'] and record['annotation']['animalType']:
        score += 0.4
    
    # Metadata completeness (20%)
    if record['metadata']['location'] and record['metadata']['captureDate']:
        score += 0.2
    
    # Expert validation (10%)
    if record['validation']['status'] == 'validated':
        score += 0.1
    
    return score

def check_image_quality(image_data):
    """Check if image meets quality standards"""
    try:
        # Decode base64 image
        image_bytes = base64.b64decode(image_data.split(',')[1])
        image = Image.open(io.BytesIO(image_bytes))
        
        # Check resolution (minimum 512x512)
        width, height = image.size
        if width < 512 or height < 512:
            return False
        
        # Check aspect ratio (between 0.5 and 2.0)
        aspect_ratio = width / height
        if aspect_ratio < 0.5 or aspect_ratio > 2.0:
            return False
        
        return True
    except Exception:
        return False

if __name__ == "__main__":
    validate_training_data()
```

## üéØ Phase 3: Custom Model Training (Weeks 5-12)

### Step 1: Training Infrastructure Setup

```bash
# Install training dependencies
pip install torch torchvision transformers
pip install timm efficientnet_pytorch
pip install albumentations opencv-python
pip install wandb tensorboard
pip install optuna ray[tune]

# Setup training directory structure
mkdir -p training/{data,models,logs,configs}
mkdir -p training/data/{train,val,test}
```

### Step 2: Training Script

```python
# training/train_livestock_model.py
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
import torchvision.transforms as transforms
from efficientnet_pytorch import EfficientNet
import json
import os
from PIL import Image
import base64
import io

class LivestockDataset(Dataset):
    def __init__(self, data_dir, transform=None):
        self.data_dir = data_dir
        self.transform = transform
        self.samples = []
        
        # Load all training samples
        for filename in os.listdir(data_dir):
            if filename.endswith('.json'):
                with open(os.path.join(data_dir, filename), 'r') as f:
                    record = json.load(f)
                    if record['validation']['status'] == 'validated':
                        self.samples.append(record)
        
        # Create breed to index mapping
        breeds = set(sample['annotation']['breed'] for sample in self.samples)
        self.breed_to_idx = {breed: idx for idx, breed in enumerate(sorted(breeds))}
        self.idx_to_breed = {idx: breed for breed, idx in self.breed_to_idx.items()}
        
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # Decode image
        image_data = sample['imageData']
        image_bytes = base64.b64decode(image_data.split(',')[1])
        image = Image.open(io.BytesIO(image_bytes)).convert('RGB')
        
        if self.transform:
            image = self.transform(image)
        
        # Get labels
        breed_label = self.breed_to_idx[sample['annotation']['breed']]
        animal_type_label = 1 if sample['annotation']['animalType'] == 'buffalo' else 0
        
        return {
            'image': image,
            'breed': torch.tensor(breed_label, dtype=torch.long),
            'animal_type': torch.tensor(animal_type_label, dtype=torch.long),
            'metadata': sample['metadata']
        }

class LivestockClassifier(nn.Module):
    def __init__(self, num_breeds, num_animal_types=2):
        super().__init__()
        
        # Use EfficientNet-B4 as backbone
        self.backbone = EfficientNet.from_pretrained('efficientnet-b4')
        
        # Replace classifier with custom heads
        in_features = self.backbone._fc.in_features
        self.backbone._fc = nn.Identity()  # Remove original classifier
        
        # Multi-task heads
        self.breed_head = nn.Sequential(
            nn.Dropout(0.3),
            nn.Linear(in_features, 512),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(512, num_breeds)
        )
        
        self.animal_type_head = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(in_features, 128),
            nn.ReLU(),
            nn.Linear(128, num_animal_types)
        )
    
    def forward(self, x):
        features = self.backbone(x)
        
        return {
            'breed': self.breed_head(features),
            'animal_type': self.animal_type_head(features)
        }

def train_model():
    # Data transforms
    train_transform = transforms.Compose([
        transforms.Resize((380, 380)),
        transforms.RandomCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(15),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    val_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    # Datasets
    train_dataset = LivestockDataset('data/validated', transform=train_transform)
    val_dataset = LivestockDataset('data/val', transform=val_transform)
    
    # Data loaders
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)
    
    # Model
    model = LivestockClassifier(num_breeds=len(train_dataset.breed_to_idx))
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    
    # Loss and optimizer
    breed_criterion = nn.CrossEntropyLoss()
    animal_type_criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)
    
    # Training loop
    for epoch in range(100):
        model.train()
        train_loss = 0.0
        
        for batch in train_loader:
            images = batch['image'].to(device)
            breed_labels = batch['breed'].to(device)
            animal_type_labels = batch['animal_type'].to(device)
            
            optimizer.zero_grad()
            
            outputs = model(images)
            
            # Multi-task loss
            breed_loss = breed_criterion(outputs['breed'], breed_labels)
            animal_type_loss = animal_type_criterion(outputs['animal_type'], animal_type_labels)
            
            total_loss = breed_loss + 0.3 * animal_type_loss
            total_loss.backward()
            
            optimizer.step()
            train_loss += total_loss.item()
        
        scheduler.step()
        
        # Validation
        if epoch % 5 == 0:
            val_accuracy = validate_model(model, val_loader, device)
            print(f"Epoch {epoch}: Train Loss: {train_loss/len(train_loader):.4f}, Val Accuracy: {val_accuracy:.4f}")
            
            # Save best model
            torch.save({
                'model_state_dict': model.state_dict(),
                'breed_to_idx': train_dataset.breed_to_idx,
                'epoch': epoch,
                'accuracy': val_accuracy
            }, f'models/livestock_model_epoch_{epoch}.pth')

def validate_model(model, val_loader, device):
    model.eval()
    correct = 0
    total = 0
    
    with torch.no_grad():
        for batch in val_loader:
            images = batch['image'].to(device)
            breed_labels = batch['breed'].to(device)
            
            outputs = model(images)
            _, predicted = torch.max(outputs['breed'].data, 1)
            
            total += breed_labels.size(0)
            correct += (predicted == breed_labels).sum().item()
    
    return correct / total

if __name__ == "__main__":
    train_model()
```

## üéØ Phase 4: Deployment & Integration (Weeks 13-16)

### Step 1: Model Deployment

```python
# deployment/model_server.py
from fastapi import FastAPI, File, UploadFile, HTTPException
import torch
from PIL import Image
import io
import base64
from torchvision import transforms
import json

app = FastAPI()

# Load trained model
model = None
breed_to_idx = {}
idx_to_breed = {}

def load_model():
    global model, breed_to_idx, idx_to_breed
    
    checkpoint = torch.load('models/best_model.pth', map_location='cpu')
    model = LivestockClassifier(num_breeds=len(checkpoint['breed_to_idx']))
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    breed_to_idx = checkpoint['breed_to_idx']
    idx_to_breed = {v: k for k, v in breed_to_idx.items()}

@app.on_event("startup")
async def startup_event():
    load_model()

@app.post("/classify")
async def classify_image(file: UploadFile = File(...)):
    try:
        # Read and preprocess image
        image_data = await file.read()
        image = Image.open(io.BytesIO(image_data)).convert('RGB')
        
        transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
        
        image_tensor = transform(image).unsqueeze(0)
        
        # Model inference
        with torch.no_grad():
            outputs = model(image_tensor)
            breed_probs = torch.softmax(outputs['breed'], dim=1)
            animal_type_probs = torch.softmax(outputs['animal_type'], dim=1)
        
        # Get predictions
        breed_idx = torch.argmax(breed_probs).item()
        breed_confidence = breed_probs[0, breed_idx].item()
        
        animal_type = 'buffalo' if torch.argmax(animal_type_probs).item() == 1 else 'cattle'
        animal_type_confidence = torch.max(animal_type_probs).item()
        
        # Get top 3 breeds
        top3_breeds = torch.topk(breed_probs, 3)
        top3_list = [
            {
                'breed': idx_to_breed[idx.item()],
                'confidence': conf.item()
            }
            for idx, conf in zip(top3_breeds.indices[0], top3_breeds.values[0])
        ]
        
        return {
            'animal_type': {
                'prediction': animal_type,
                'confidence': animal_type_confidence,
                'confidence_level': 'high' if animal_type_confidence > 0.8 else 'medium'
            },
            'breed': {
                'prediction': idx_to_breed[breed_idx],
                'confidence': breed_confidence,
                'confidence_level': 'high' if breed_confidence > 0.8 else 'medium',
                'top_3': top3_list,
                'needs_verification': breed_confidence < 0.7
            },
            'model_version': 'custom_v1.0',
            'processing_time': 0.1
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### Step 2: Frontend Integration

Update the frontend to use the custom model when available:

```typescript
// src/lib/services/custom-model-client.ts
export class CustomModelClient {
  private apiUrl = process.env.NEXT_PUBLIC_CUSTOM_MODEL_API || 'http://localhost:8000';
  
  async classifyImage(imageFile: File): Promise<ClassificationResult> {
    const formData = new FormData();
    formData.append('file', imageFile);
    
    const response = await fetch(`${this.apiUrl}/classify`, {
      method: 'POST',
      body: formData
    });
    
    if (!response.ok) {
      throw new Error(`Custom model API error: ${response.status}`);
    }
    
    return await response.json();
  }
  
  async isAvailable(): Promise<boolean> {
    try {
      const response = await fetch(`${this.apiUrl}/health`);
      return response.ok;
    } catch {
      return false;
    }
  }
}
```

## üéØ Success Metrics & Monitoring

### Key Performance Indicators

1. **Accuracy Progression**
   - Week 1: 85% (Enhanced current system)
   - Month 1: 90% (With data collection)
   - Month 3: 95% (Custom model)
   - Month 6: 99%+ (Optimized system)

2. **Data Collection Targets**
   - Week 1: 100 images
   - Month 1: 1,000 images
   - Month 3: 10,000 images
   - Month 6: 50,000+ images

3. **User Engagement**
   - Daily active users
   - Classification requests per day
   - User feedback submissions
   - Expert validation completion rate

### Monitoring Dashboard

```typescript
// src/components/TrainingDashboard.tsx
export function TrainingDashboard() {
  const [stats, setStats] = useState(null);
  
  useEffect(() => {
    fetchTrainingStats().then(setStats);
  }, []);
  
  return (
    <div className="training-dashboard">
      <div className="grid grid-cols-1 md:grid-cols-4 gap-6">
        <div className="stat-card">
          <h3>Current Accuracy</h3>
          <div className="text-3xl font-bold text-green-600">
            {stats?.accuracy}%
          </div>
          <div className="text-sm text-gray-500">
            +{stats?.accuracyIncrease}% this week
          </div>
        </div>
        
        <div className="stat-card">
          <h3>Training Images</h3>
          <div className="text-3xl font-bold text-blue-600">
            {stats?.totalImages?.toLocaleString()}
          </div>
          <div className="text-sm text-gray-500">
            +{stats?.newImages} this week
          </div>
        </div>
        
        <div className="stat-card">
          <h3>Expert Validations</h3>
          <div className="text-3xl font-bold text-purple-600">
            {stats?.validatedImages?.toLocaleString()}
          </div>
          <div className="text-sm text-gray-500">
            {stats?.validationRate}% validation rate
          </div>
        </div>
        
        <div className="stat-card">
          <h3>Model Version</h3>
          <div className="text-2xl font-bold text-orange-600">
            v{stats?.modelVersion}
          </div>
          <div className="text-sm text-gray-500">
            Last updated: {stats?.lastUpdate}
          </div>
        </div>
      </div>
      
      <div className="mt-8 grid grid-cols-1 lg:grid-cols-2 gap-6">
        <div className="chart-container">
          <h3>Accuracy Over Time</h3>
          {/* Add Chart.js or similar chart component */}
        </div>
        
        <div className="chart-container">
          <h3>Data Collection Progress</h3>
          {/* Add progress chart */}
        </div>
      </div>
    </div>
  );
}
```

## üöÄ Getting Started Today

1. **Immediate (Today)**:
   ```bash
   # Integrate enhanced classification
   npm install
   # Add user location detection
   # Implement feedback collection
   ```

2. **This Week**:
   - Set up data collection forms
   - Create expert validation interface
   - Start collecting training data
   - Partner with local veterinarians

3. **Next Month**:
   - Begin custom model training
   - Set up training infrastructure
   - Collect 1,000+ validated images
   - Implement quality control pipeline

4. **Ongoing**:
   - Continuous data collection
   - Regular model retraining
   - Performance monitoring
   - Expert network expansion

This systematic approach will progressively improve your AI accuracy from 75% to 99%+ while building a sustainable, continuously learning system that gets better with every use.